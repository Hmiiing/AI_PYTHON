
# tensor manipulation


```python
import numpy as np
import torch
```


```python
t=np.array([0.,1.,2.,3.,4.,5.,6.])
print(t)
```

    [0. 1. 2. 3. 4. 5. 6.]
    


```python
t.ndim
```




    1




```python
t.shape
```




    (7,)




```python
t[0]
```




    0.0




```python
t.size
```




    7




```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
print(t)
```

    tensor([[ 1.,  2.,  3.],
            [ 4.,  5.,  6.],
            [ 7.,  8.,  9.],
            [10., 11., 12.]])
    


```python
t.dim()
```




    2




```python
t.size()
```




    torch.Size([4, 3])




```python
t.shape
```




    torch.Size([4, 3])




```python
print()
print('-------------')
print('Mul vs Matmul')
print('-------------')
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1

m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1 * m2) # 2 x 2
print(m1.mul(m2))
```

    
    -------------
    Mul vs Matmul
    -------------
    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[ 5.],
            [11.]])
    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[1., 2.],
            [6., 8.]])
    tensor([[1., 2.],
            [6., 8.]])
    


```python
# Same shape
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)
```

    tensor([[5., 5.]])
    


```python
t = torch.FloatTensor([1, 2])
print(t.mean())
```

    tensor(1.5000)
    


```python
# Can't use mean() on integers
t = torch.LongTensor([1, 2])
try:
    print(t.mean())
except Exception as exc:
    print(exc)
```

    Can only calculate the mean of floating types. Got Long instead.
    


```python
print(t.sum())
```

    tensor(3)
    


```python
t.max()
```




    tensor(2)




```python
t = np.array([[[0, 1, 2],
               [3, 4, 5]],

              [[6, 7, 8],
               [9, 10, 11]]])
ft = torch.FloatTensor(t)
print(ft.shape)
```

    torch.Size([2, 2, 3])
    


```python
print(ft.view([-1, 3]))
print(ft.view([-1, 3]).shape)
```

    tensor([[ 0.,  1.,  2.],
            [ 3.,  4.,  5.],
            [ 6.,  7.,  8.],
            [ 9., 10., 11.]])
    torch.Size([4, 3])
    


```python
print(ft.squeeze())
print(ft.squeeze().shape)
```

    tensor([[[ 0.,  1.,  2.],
             [ 3.,  4.,  5.]],
    
            [[ 6.,  7.,  8.],
             [ 9., 10., 11.]]])
    torch.Size([2, 2, 3])
    


```python
print(ft.unsqueeze(0))
print(ft.unsqueeze(0).shape)
```

    tensor([[[[ 0.,  1.,  2.],
              [ 3.,  4.,  5.]],
    
             [[ 6.,  7.,  8.],
              [ 9., 10., 11.]]]])
    torch.Size([1, 2, 2, 3])
    


```python
lt = torch.LongTensor([[0], [1], [2], [0]])
print(lt)
```

    tensor([[0],
            [1],
            [2],
            [0]])
    


```python
one_hot = torch.zeros(4, 3) # batch_size = 4, classes = 3
one_hot.scatter_(1, lt, 1)
print(one_hot)
```

    tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.],
            [1., 0., 0.]])
    


```python
lt = torch.LongTensor([1, 2, 3, 4])
print(lt)
print(lt.float())
bt = torch.ByteTensor([True, False, False, True])
print(bt)
print(bt.long())
print(bt.float())
```

    tensor([1, 2, 3, 4])
    tensor([1., 2., 3., 4.])
    tensor([1, 0, 0, 1], dtype=torch.uint8)
    tensor([1, 0, 0, 1])
    tensor([1., 0., 0., 1.])
    


```python
x = torch.FloatTensor([[1, 2], [3, 4]])
y = torch.FloatTensor([[5, 6], [7, 8]])
print(torch.cat([x, y], dim=0))
print(torch.cat([x, y], dim=1))
```

    tensor([[1., 2.],
            [3., 4.],
            [5., 6.],
            [7., 8.]])
    tensor([[1., 2., 5., 6.],
            [3., 4., 7., 8.]])
    


```python
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])
print(torch.stack([x, y, z]))
print(torch.stack([x, y, z], dim=1))
print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))
      
```

    tensor([[1., 4.],
            [2., 5.],
            [3., 6.]])
    tensor([[1., 2., 3.],
            [4., 5., 6.]])
    tensor([[1., 4.],
            [2., 5.],
            [3., 6.]])
    


```python
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
print(x)
print(torch.ones_like(x))
print(torch.zeros_like(x))
```

    tensor([[0., 1., 2.],
            [2., 1., 0.]])
    tensor([[1., 1., 1.],
            [1., 1., 1.]])
    tensor([[0., 0., 0.],
            [0., 0., 0.]])
    


```python
x = torch.FloatTensor([[1, 2], [3, 4]])
print(x.mul(2.))
print(x)
print(x.mul_(2.))
print(x)
```

    tensor([[2., 4.],
            [6., 8.]])
    tensor([[1., 2.],
            [3., 4.]])
    tensor([[2., 4.],
            [6., 8.]])
    tensor([[2., 4.],
            [6., 8.]])
    


```python
for x, y in zip([1, 2, 3], [4, 5, 6]):
    print(x, y)
```

    1 4
    2 5
    3 6
    

# 1. linear regression


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```


```python
# data
x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[2],[4],[6]])
# 모델 초기화
W=torch.zeros(1,requires_grad=True)
b=torch.zeros(1,requires_grad=True)
#optimizer 설정
optimizer=optim.SGD([W,b],lr=0.01)
nb_epochs = 1000
for epoch in range(nb_epochs+1):
    #가설설정
    hypothesis = x_train*W+b
    #cost계산
    cost=F.mse_loss(hypothesis,y_train)
    #개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    #로그 출력
    if epoch % 100 ==0:
        print('Epoch:{:4d}/{},W:{:.3f},b:{:.3f},Cost:{:.3f}'.format(epoch,nb_epochs,W.item(),b.item(),cost.item()))
    
```

    Epoch:   0/1000,W:0.187,b:0.080,Cost:18.667
    Epoch: 100/1000,W:1.746,b:0.578,Cost:0.048
    Epoch: 200/1000,W:1.800,b:0.454,Cost:0.030
    Epoch: 300/1000,W:1.843,b:0.357,Cost:0.018
    Epoch: 400/1000,W:1.876,b:0.281,Cost:0.011
    Epoch: 500/1000,W:1.903,b:0.221,Cost:0.007
    Epoch: 600/1000,W:1.924,b:0.174,Cost:0.004
    Epoch: 700/1000,W:1.940,b:0.136,Cost:0.003
    Epoch: 800/1000,W:1.953,b:0.107,Cost:0.002
    Epoch: 900/1000,W:1.963,b:0.084,Cost:0.001
    Epoch:1000/1000,W:1.971,b:0.066,Cost:0.001
    


```python
#예측해보기
x_test=torch.FloatTensor([[4],[5],[6]])
predict=x_test*W+b
predict
```




    tensor([[ 7.9497],
            [ 9.9205],
            [11.8914]], grad_fn=<AddBackward0>)



### high level


```python
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1,1)
        
    def forward(self,x):
        return self.linear(x)
```


```python
#데이터
x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[2],[4],[6]])
#모델초기화
model=LinearRegressionModel()
#optimizer설정
optimizer=optim.SGD(model.parameters(),lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs+1):
    prediction = model(x_train)
    cost = F.mse_loss(prediction,y_train)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch % 100==0:
        params=list(model.parameters())
        W=params[0].item()
        b=params[1].item()
        print('Epoch:{:4d}/{}, W:{:.3f},b:{:.3f},Cost:{}'.format(epoch,nb_epochs,W,b,cost.item()))
```

    Epoch:   0/1000, W:-0.210,b:0.175,Cost:26.888227462768555
    Epoch: 100/1000, W:1.668,b:0.756,Cost:0.08232402056455612
    Epoch: 200/1000, W:1.739,b:0.594,Cost:0.050871238112449646
    Epoch: 300/1000, W:1.795,b:0.467,Cost:0.03143530339002609
    Epoch: 400/1000, W:1.839,b:0.367,Cost:0.01942511647939682
    Epoch: 500/1000, W:1.873,b:0.289,Cost:0.012003554031252861
    Epoch: 600/1000, W:1.900,b:0.227,Cost:0.0074174487963318825
    Epoch: 700/1000, W:1.922,b:0.178,Cost:0.004583548288792372
    Epoch: 800/1000, W:1.938,b:0.140,Cost:0.002832368016242981
    Epoch: 900/1000, W:1.952,b:0.110,Cost:0.001750220893882215
    Epoch:1000/1000, W:1.962,b:0.087,Cost:0.0010815331479534507
    


```python
#예측하기
x_test = torch.FloatTensor([[4],[5],[6]])
predict = x_test * W + b
predict
```




    tensor([[ 7.9342],
            [ 9.8961],
            [11.8580]])




```python
# gradient
x_train=torch.FloatTensor([[1],[2],[3]])
y_train=torch.FloatTensor([[2],[4],[6]])
W=torch.zeros(1)
lr=0.1
nb_epochs=10
for epoch in range(nb_epochs+1):
    hypothesis=W*x_train
    cost=F.mse_loss(hypothesis,y_train)
    gradient=torch.sum((W*x_train-y_train)*x_train)
    print('Epoch : {:4d}/{}, W:{:.3f},Cost:{:.6f}'.format(epoch,nb_epochs,W.item(),cost.item()))
    W -= lr*gradient
```

    Epoch :    0/10, W:0.000,Cost:18.666666
    Epoch :    1/10, W:2.800,Cost:2.986666
    Epoch :    2/10, W:1.680,Cost:0.477867
    Epoch :    3/10, W:2.128,Cost:0.076459
    Epoch :    4/10, W:1.949,Cost:0.012233
    Epoch :    5/10, W:2.020,Cost:0.001957
    Epoch :    6/10, W:1.992,Cost:0.000313
    Epoch :    7/10, W:2.003,Cost:0.000050
    Epoch :    8/10, W:1.999,Cost:0.000008
    Epoch :    9/10, W:2.001,Cost:0.000001
    Epoch :   10/10, W:2.000,Cost:0.000000
    

### multivariate linear regression


```python
# data
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
#초기화
W1=torch.zeros(1,requires_grad=True)
W2=torch.zeros(1,requires_grad=True)
W3=torch.zeros(1,requires_grad=True)
b=torch.zeros(1,requires_grad=True)
#optimizer
optimizer=optim.SGD([W1,W2,W3,b],lr=1e-5)
nb_epochs = 1000
for epoch in range(nb_epochs+1):
    hypothesis  = x1_train*W1+x2_train*W2+x3_train*W3+b
    cost=F.mse_loss(hypothesis,y_train)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch % 100 ==0:
        print('Epcoh : {:4d}/{}, W1:{:.3f},W2:{:.3f},W3:{:.3f},b:{:.3f},Cost:{:.6f}'.format(epoch,nb_epochs,W1.item(),W2.item(),W3.item(),b.item(),cost.item()))
```

    Epcoh :    0/1000, W1:0.294,W2:0.294,W3:0.297,b:0.003,Cost:29661.800781
    Epcoh :  100/1000, W1:0.674,W2:0.661,W3:0.676,b:0.008,Cost:1.563634
    Epcoh :  200/1000, W1:0.679,W2:0.655,W3:0.677,b:0.008,Cost:1.497603
    Epcoh :  300/1000, W1:0.684,W2:0.649,W3:0.677,b:0.008,Cost:1.435026
    Epcoh :  400/1000, W1:0.689,W2:0.643,W3:0.678,b:0.008,Cost:1.375730
    Epcoh :  500/1000, W1:0.694,W2:0.638,W3:0.678,b:0.009,Cost:1.319503
    Epcoh :  600/1000, W1:0.699,W2:0.633,W3:0.679,b:0.009,Cost:1.266215
    Epcoh :  700/1000, W1:0.704,W2:0.627,W3:0.679,b:0.009,Cost:1.215693
    Epcoh :  800/1000, W1:0.709,W2:0.622,W3:0.679,b:0.009,Cost:1.167821
    Epcoh :  900/1000, W1:0.713,W2:0.617,W3:0.680,b:0.009,Cost:1.122419
    Epcoh : 1000/1000, W1:0.718,W2:0.613,W3:0.680,b:0.009,Cost:1.079375
    


```python
# data
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
W=torch.zeros((3,1),requires_grad=True)
b=torch.zeros(1,requires_grad=True)
optimizer = optim.SGD([W,b],lr=1e-5)
nb_epochs = 20
for epoch in range(nb_epochs+1):
    hypothesis = x_train.matmul(W)+b     # 속도가 빨라짐
    cost = torch.mean((hypothesis - y_train)**2)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
    ))
```

    Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781
    Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508
    Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712646
    Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527
    Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005
    Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371017
    Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139
    Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305
    Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228
    Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135
    Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688
    Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541
    Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651413
    Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387
    Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923
    Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412
    Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141
    Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253
    Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500
    Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770
    Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033
    

### nn.Module


```python
class MultivariateLinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear=nn.Linear(3,1)
    def forward(self,x):
        return self.linear(x)
model = MultivariateLinearRegressionModel()
optimizer=optim.SGD([W,b],lr=1e-5)
nb_epochs = 20
for epoch in range(nb_epochs+1):
    hypothesis = model(x_train)
    cost=F.mse_loss(hypothesis,y_train)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    print('Epoch {:4d}/{} hypothesis : {} Cost : {:.6f}'.format(epoch,nb_epochs,hypothesis.squeeze().detach(),cost.item()))
```

    Epoch    0/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    1/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    2/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    3/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    4/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    5/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    6/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    7/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    8/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch    9/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   10/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   11/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   12/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   13/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   14/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   15/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   16/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   17/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   18/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   19/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    Epoch   20/20 hypothesis : tensor([-64.4287, -78.7710, -76.8811, -83.6950, -60.5436]) Cost : 60331.523438
    

### loading data


```python
from torch.utils.data import Dataset
class CustomDataset(Dataset):
    def __init__(self):
        self.x_data=[[73,80,75],
                    [93,88,93],
                    [89,91,90],
                    [73,66,70]]
        self.y_data=[[152],[185],[180],[196],[142]]
    def __len__(self):
        return len(self.x_data)
    def __getitem__(self,idx):
        x=torch.FloatTensor(self.x_data[idx])
        y=torch.FloatTensor(self.y_data[idx])
        return(x,y)
        
dataset=CustomDataset()
```


```python
from torch.utils.data import DataLoader
dataloader = DataLoader(
dataset,
batch_size=2,
shuffle=True
)
```


```python
class MultivariateLinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3,1)
    def forward(self,x):
        return self.linear(x)
model = MultivariateLinearRegressionModel()
# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1e-5)
nb_epochs = 20
for epoch in range(nb_epochs+1):
    for batch_idx, samples in enumerate(dataloader):   # 배치 사이즈
        x_train, y_train = samples
        prediction = model(x_train)
        cost = F.mse_loss(prediction,y_train)
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        print('Epoch {:4d}/{} Batch {}/{} Cost:{:.6f}'.format(epoch, nb_epochs, batch_idx+1,len(dataloader),cost.item()))
```

    Epoch    0/20 Batch 1/2 Cost:35401.679688
    Epoch    0/20 Batch 2/2 Cost:42467.171875
    Epoch    1/20 Batch 1/2 Cost:41328.203125
    Epoch    1/20 Batch 2/2 Cost:36540.648438
    Epoch    2/20 Batch 1/2 Cost:43884.644531
    Epoch    2/20 Batch 2/2 Cost:33984.207031
    Epoch    3/20 Batch 1/2 Cost:43884.644531
    Epoch    3/20 Batch 2/2 Cost:33984.207031
    Epoch    4/20 Batch 1/2 Cost:43884.644531
    Epoch    4/20 Batch 2/2 Cost:33984.207031
    Epoch    5/20 Batch 1/2 Cost:41328.203125
    Epoch    5/20 Batch 2/2 Cost:36540.648438
    Epoch    6/20 Batch 1/2 Cost:36540.648438
    Epoch    6/20 Batch 2/2 Cost:41328.203125
    Epoch    7/20 Batch 1/2 Cost:43884.644531
    Epoch    7/20 Batch 2/2 Cost:33984.207031
    Epoch    8/20 Batch 1/2 Cost:43884.644531
    Epoch    8/20 Batch 2/2 Cost:33984.207031
    Epoch    9/20 Batch 1/2 Cost:43884.644531
    Epoch    9/20 Batch 2/2 Cost:33984.207031
    Epoch   10/20 Batch 1/2 Cost:33984.207031
    Epoch   10/20 Batch 2/2 Cost:43884.644531
    Epoch   11/20 Batch 1/2 Cost:41328.203125
    Epoch   11/20 Batch 2/2 Cost:36540.648438
    Epoch   12/20 Batch 1/2 Cost:36540.648438
    Epoch   12/20 Batch 2/2 Cost:41328.203125
    Epoch   13/20 Batch 1/2 Cost:33984.207031
    Epoch   13/20 Batch 2/2 Cost:43884.644531
    Epoch   14/20 Batch 1/2 Cost:33984.207031
    Epoch   14/20 Batch 2/2 Cost:43884.644531
    Epoch   15/20 Batch 1/2 Cost:36540.648438
    Epoch   15/20 Batch 2/2 Cost:41328.203125
    Epoch   16/20 Batch 1/2 Cost:36540.648438
    Epoch   16/20 Batch 2/2 Cost:41328.203125
    Epoch   17/20 Batch 1/2 Cost:42467.171875
    Epoch   17/20 Batch 2/2 Cost:35401.679688
    Epoch   18/20 Batch 1/2 Cost:43884.644531
    Epoch   18/20 Batch 2/2 Cost:33984.207031
    Epoch   19/20 Batch 1/2 Cost:33984.207031
    Epoch   19/20 Batch 2/2 Cost:43884.644531
    Epoch   20/20 Batch 1/2 Cost:33984.207031
    Epoch   20/20 Batch 2/2 Cost:43884.644531
    

### logistic regression


```python
torch.manual_seed(1)
```




    <torch._C.Generator at 0x2545f71d070>




```python
x_data=[[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]
y_data=[[0],[0],[0],[1],[1],[1]]
```


```python
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)
print(x_train)
print(y_train)
x_train.shape
```

    tensor([[1., 2.],
            [2., 3.],
            [3., 1.],
            [4., 3.],
            [5., 3.],
            [6., 2.]])
    tensor([[0.],
            [0.],
            [0.],
            [1.],
            [1.],
            [1.]])
    




    torch.Size([6, 2])




```python
W=torch.zeros((2,1),requires_grad=True)
b=torch.zeros(1,requires_grad=True)
optimizer=optim.SGD([W,b],lr=1)
nb_epochs=1000

for epoch in range(nb_epochs+1):
    hypothesis=torch.sigmoid(x_train.matmul(W)+b)
    cost=F.binary_cross_entropy(hypothesis,y_train)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch %100==0:
        print('Epoch {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))
```

    Epoch    0/1000 Cost : 0.693147
    Epoch  100/1000 Cost : 0.134722
    Epoch  200/1000 Cost : 0.080643
    Epoch  300/1000 Cost : 0.057900
    Epoch  400/1000 Cost : 0.045300
    Epoch  500/1000 Cost : 0.037261
    Epoch  600/1000 Cost : 0.031672
    Epoch  700/1000 Cost : 0.027556
    Epoch  800/1000 Cost : 0.024394
    Epoch  900/1000 Cost : 0.021888
    Epoch 1000/1000 Cost : 0.019852
    


```python
prediction = hypothesis >= torch.FloatTensor([0.5])
correct_prediction = prediction.float()==y_train
correct_prediction
```




    tensor([[1],
            [1],
            [1],
            [1],
            [1],
            [1]], dtype=torch.uint8)



# higher implementation with class


```python
class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear=nn.Linear(2,1)
        self.sigmoid=nn.Sigmoid()
    def forward(self,x):
        return self.sigmoid(self.linear(x))
model=BinaryClassifier()
```


```python
optimizer=optim.SGD(model.parameters(),lr=1)
nb_epochs=100
for epoch in range(nb_epochs+1):
    hypothesis = model(x_train)
    cost=F.binary_cross_entropy(hypothesis,y_train)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch %10==0:
        prediction = hypothesis >=torch.FloatTensor([0.5])
        correct_prediction = prediction.float()==y_train
        accuracy = correct_prediction.sum().item()/len(correct_prediction)
        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(epoch,nb_epochs,cost.item(),accuracy*100))
            
```

    Epoch    0/100 Cost: 0.539713 Accuracy 83.33%
    Epoch   10/100 Cost: 0.614853 Accuracy 66.67%
    Epoch   20/100 Cost: 0.441875 Accuracy 66.67%
    Epoch   30/100 Cost: 0.373145 Accuracy 83.33%
    Epoch   40/100 Cost: 0.316358 Accuracy 83.33%
    Epoch   50/100 Cost: 0.266094 Accuracy 83.33%
    Epoch   60/100 Cost: 0.220498 Accuracy 100.00%
    Epoch   70/100 Cost: 0.182095 Accuracy 100.00%
    Epoch   80/100 Cost: 0.157299 Accuracy 100.00%
    Epoch   90/100 Cost: 0.144091 Accuracy 100.00%
    Epoch  100/100 Cost: 0.134272 Accuracy 100.00%
    

### softmax classification


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```


```python
x_train = [[1, 2, 1, 1],
           [2, 1, 3, 2],
           [3, 1, 3, 4],
           [4, 1, 5, 5],
           [1, 7, 5, 5],
           [1, 2, 5, 6],
           [1, 6, 6, 6],
           [1, 7, 7, 7]]
y_train = [2, 2, 2, 1, 1, 1, 0, 0]
x_train = torch.FloatTensor(x_train)
y_train = torch.LongTensor(y_train)
```


```python
#모델초기화
W=torch.zeros((4,3),requires_grad=True)
b=torch.zeros(1,requires_grad=True)
#optimizer 설정
optimizer=optim.SGD([W,b],lr=0.1)
nb_epochs = 1000
for epoch in range(nb_epochs+1):
    #cost
    hypothesis=F.softmax(x_train.matmul(W)+b,dim=1)
    y_one_hot=torch.zeros_like(hypothesis)
    y_one_hot.scatter_(1,y_train.unsqueeze(1),1)
    cost=(y_one_hot*-torch.log(F.softmax(hypothesis,dim=1))).sum(dim=1).mean()
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, cost.item()
        ))
```

    Epoch    0/1000 Cost: 1.098612
    Epoch  100/1000 Cost: 0.901535
    Epoch  200/1000 Cost: 0.839114
    Epoch  300/1000 Cost: 0.807826
    Epoch  400/1000 Cost: 0.788472
    Epoch  500/1000 Cost: 0.774822
    Epoch  600/1000 Cost: 0.764449
    Epoch  700/1000 Cost: 0.756191
    Epoch  800/1000 Cost: 0.749398
    Epoch  900/1000 Cost: 0.743671
    Epoch 1000/1000 Cost: 0.738749
    


```python

```
